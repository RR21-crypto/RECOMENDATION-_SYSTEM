# -*- coding: utf-8 -*-
"""Proyek Akhir : Membuat Model Sistem Rekomendasi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qy7j45lzL5d39R9Y14yd_uOjEYOBnsM6

### ACKNOWLEGMENT

projek ini menggunakna data dari movielens.movielens merupakan sebuah grup peneliti yang menyediakan data set mengenai movie yang gratis dengan tujuan sebagai alat belajar dan penelitian

link : https://files.grouplens.org/datasets/movielens/ml-latest-small.zip

# Data Understanding

## Data Loading
dalam tahap ini , data akan di unduh dengan menggunakan link di bawah .setelah di di unduh , data akan di lihat setiap feature dan mencoba untuk memahami setiap file yang ada
"""

# mengunduh datset
!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip

# membuka file dari bentuk ZIP menjadi bentuk CSV
!unzip /content/ml-latest-small.zip

"""## Explanatory Data"""

# mengimport data dan menentukan variable setiap file
import pandas as pd
film = pd.read_csv('/content/ml-latest-small/movies.csv')
rating =pd.read_csv('/content/ml-latest-small/ratings.csv')
tag =pd.read_csv ('/content/ml-latest-small/tags.csv')
link = pd.read_csv('/content/ml-latest-small/links.csv')

# info mengenai data set dari Film
film.info()

# info mengenai data set dari rating
rating.info()

# info mengenai data set dari tag
tag.info()

# info mengenai data set dari link
link.info()

print('jumlah film yang ada pada dataset adalah :',len (film.movieId.unique()))
print ('jumlah tag yang ada pada dataset ini adalah : ',len(tag.userId.unique()))
print('jumlah user yang memberikan rating :',len(rating.userId.unique()))
print('jumah link  yang ada pada datsaset ini adalah',len(link.movieId.unique()))

"""dari informasi diatas jumlah data cukup untuk di olah menjadi sistem rekomendasi dengan jumlah film berjumlah 9742 film.

# Data Preprocessing
## Menggabungkan semua film
"""

import numpy as np

# Menggabungkan seluruh movie_Id
movie_Id = np.concatenate((
    film.movieId.unique(),
    link.movieId.unique(),
    tag.movieId.unique()
))

# Mengurutkan data dan menghapus data yang sama
movie_Id = np.sort(np.unique(movie_Id))

print('Jumlah seluruh data movie Id yang ada adalah : ', len(movie_Id))

movie_Id

"""## Menggabungkan Seluruh User"""

# Menggabungkan seluruh user_Id
user_Id = np.concatenate((
    tag.userId.unique(),
    rating.userId.unique()

)) ,
# mengurutkan data dan menghapus data yang sama
user_Id = np.sort(np.unique(user_Id))
print('jumlah data set yang memiliki user Id adalah : ',len(user_Id))

user_Id

"""seperti yang terlihat bahwa tittle memiliki tahun rilis , untuk mengurangi eror di karenakan terdapat angka dan huruf pada satu cell, sebaiknya di pisah dengan tahapan di bawah"""

# membuat satu feature baru untuk tahun rilis film agar informasi tidak hilang ketika kita membersihkan fitur title
film['tahun_rilis'] = film.title.str.extract('([0-9]{4})')
film.head()

# ubah  column menjadi string
film['title'] = film['title'].astype(str)

#hapus keterangan tahun pada  fitur titte
film['title'] = film['title'].str.split('(', 1).str[0].str.strip()
film.head()

# gabungkan file film dengan file rating
films = pd.merge(film, rating, on='movieId', how='left')
# mengecek apakah sudah rapi ata belum
films

# mencek nilai rating  dan skalanya
films.rating.unique()

"""terlihat nilainya tidak bulat sehingga bagusnya di ubah menjadi nilai bulat"""

# mengubah nilai rating memnjadi nilai bulat

films['rating'] = films['rating'].apply(np.ceil)
films.rating.unique()

# hasil mengubah nilai rating
films

"""dari atas terlihat bagian yang masih sulit dipahami adalah bagian timestamp yang mana dapat dirapikan dengan menggunakan code di bawah"""

#perbaiki timestamp
import datetime

films.timestamp = pd.to_datetime(films['timestamp'], unit='s')
films.head()

#mencek apakah nilai kosong
films.isnull().sum()

"""setelah melihat nilai yang kosong sangat sedikit jika di bandingkan dengan  jumlah data yang kita miliki, pilihan paling effisien adalah pilihan untuk menghapus data tersebut seperti yang di lakukan di bawah"""

# menghapus baris yang memiliki nilai kosong
film_clean = films.dropna()
film_clean

# mencek kembali hasil pembersihan
film_clean.isnull().sum()

# Mengurutkan film berdasarkan movie_ id kemudian memasukkannya ke dalam variabel film_fix
film_fix = film_clean.sort_values('movieId', ascending=True)
film_fix

#melihat jumlah film setelah di lakukukan pembersihan

len(film_fix.movieId.unique())

#menampilkan genre yang terdapat datset ini
import sys
np.set_printoptions(threshold=sys.maxsize)
film_fix.genres.unique()

"""setelah di cek ternyata ada film yang tidak memiliki genre, hal ini tentu akan menjadi masalah di masa depan jika tidak kita tangani , untuk menangani mendrop film yang tak memilik genre , dengan menggunakan code di bawah"""

#menampilan  daftar film dengan  no genre listed
film_fix[film_fix['genres']=='(no genres listed)']

# mendrop / menghapus film yang memiliki keterangan genre no genres listed
film = films[films['genres']!='(no genres listed)']
film

# mengubah nama variable dan mengurutkan berdasarkan movie_Id kembali
preparation = film
preparation.sort_values('movieId')

#menghapus duplikat film berdasarkan Movie_ id dengan tujuan membuat model  lebih bersih dan ringan
preparation = preparation.drop_duplicates('movieId')
preparation

# hasil dari peroses sebelumnya
print('jumlah data yang dimiliki setelah di peroses : ', len(preparation))

#mengubah movie id menjadi to list
movieId= preparation['movieId'].tolist()

# menubah title menjadi dalam bentuk tolist
title =preparation['title'].tolist()

# mengubah genre menjadi dalam bentuk to list

genre = preparation ['genres'].tolist()

# mengubah tahun rilis menjadi bentuk tolist

tahun_rilis = preparation ['tahun_rilis'].tolist()

# membuat data drame baru dengan menggunkan variable di bawah
saringan =  pd.DataFrame({
    'id': movieId,
    'judul': title,
    'genre': genre,

})
saringan

"""setelah berbagai peroses pengolahan , mendapatkan hasil 9708 data . data ini siap untuk diolah menjadi model rekomendasi yang akan di kembangkan.

## Model Development dengan Content Based Filtering
"""

# membuat nama variable baru untuk model ini , agar variable saringan tak terganggu dan dapat digunakan lagi
data = saringan
data.sample(5)

"""### TF-IDF Vectorizer

proses term frequency-inverse document frequency (TF-IDF) untuk mencari kata yang penting dalam kolom genre. setelah melakukan perhitungan idf akan didapatkan index. kemudian, saya akan mencoba melakukan mapping untuk menampilkan data genre-nya.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data genre
tf.fit(data['genre'])

# Mapping array dari fitur index integer ke fitur genre
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['genre'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""hasil yang di dapatkan adalah 9708 dataset dan 21 genre  dalam model kali ini"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

## menampilkan bentuk matrix untuk menujukan korelasi film dengan genrenya
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.judul
).sample(20, axis=1).sample(10, axis=0)

"""## Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul film
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['judul'], columns=data['judul'])
print('Shape:', cosine_sim_df.shape)

# show similarity matrix
cosine_sim_df.sample(20, axis=1).sample(20, axis=0)

"""## Mendapatkan Rekomendasi

membuat fungsi untuk menampilkan top 8 film yang bisa di rekomendasikan sesuai dengan genre
"""

## membuat fungsi khusus untuk menampilkan rekomendasi film

def movie_recommendations(nama_film, similarity_data=cosine_sim_df, items=data[['judul', 'genre']], k=8):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama_film : tipe data string (str)
                judul Film (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan film sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_film].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_film agar judul film  yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_film, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

#memastikan judul film ada dalam list rekomendasi
data[data.judul.eq('Oh, God! Book II')]

# Mendapatkan rekomendasi restoran yang mirip dengan Oh, God! Book II
movie_recommendations('Oh, God! Book II')

"""sistem menghasilkan yang sangat akurat jika berdasarkan genre, dari ke 8 rekomendasi ,semua memiliki hasil genre yang sama yaitu comedy.

# Model Development dengan Collaborative Filtering
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""### Data preparation

"""

# mendekalarasikan variable baru dari file rating , dengan tujuan untuk  digunakan membuat sistem rekomendasi

dd = pd.read_csv('/content/ml-latest-small/ratings.csv')
"""
untuk menghemat waktu disini langsung saja kita menghapus nilai value yang kosong,
sehingga daat terlihat hasil di bawah  """

dd = dd.dropna()
dd.isnull().sum()

"""melihat hasil pengolahan dan masil terdapat nilai rating dan time stamp yang tak sesuai. di rating terdapat nilai tak bulat dan di timestamp format penulisan yang membingungkan,"""

dd

# membulatkan nilai rating
dd['rating'] = dd['rating'].apply(np.ceil)
dd

"""disini nilai time stamp dapat di ignore dikarenakan tidak akan di gunakan pada proses selanjutnya .
di tahap selanjutnya kita akan mepping User_ id dan rating.
"""

# change unique values of 'userId' to list
user_ids = dd['userId'].unique().tolist()
print('list userID: ', user_ids)

# encode 'userId'
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# encoding index to 'userId'
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# change unique values of 'movieId' to list
films_ids = dd['movieId'].unique().tolist()

# encode 'movieId'
films_to_films_encoded = {x: i for i, x in enumerate(films_ids)}

# encoding index to 'movieId'
films_encoded_to_films = {i: x for i, x in enumerate(films_ids)}

# Mapping 'userId' to dataframe
dd['user'] = dd['userId'].map(user_to_user_encoded)

# Mapping 'movieId' ke dataframe
dd['films'] = dd['movieId'].map(films_to_films_encoded)

# get number of users
num_users = len(user_to_user_encoded)
print(num_users)

# get number of films
num_films = len(films_encoded_to_films)
print(num_films)

# change dtype
dd['rating'] = dd['rating'].values.astype(np.float32)

# get min values of rating
min_rating = min(dd['rating'])

# get max values of rating
max_rating = max(dd['rating'])

print('Number of User: {}, Number of Films: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_films, min_rating, max_rating
))

"""kita  akan membagi data set menjadi rasio 80 persen train dan 20 persen test , sebelumnya kiat akan mengacak data set kita sehingga penyebaran akan merata dan membuat model menghasilkan hasil yang lebih konsisten dan lebih baik."""

# Mengacak dataset

dd = dd.sample(frac=1, random_state=42)

dd

# mapping users and films data into one value
x = dd[['user', 'films']].values

# ratings
y = dd['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# split data train and validation with 80/20 composition
train_indices = int(0.8 * dd.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Proses Training

mengembangkan model untuk menghitung skor kecocokan antara users dan films menggunakan teknik embedding
"""

# class recommendations
class RecommenderNet(tf.keras.Model):

  # __init__
  def __init__(self, num_users, num_films, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_films = num_films
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.films_embedding = layers.Embedding( # layer embeddings films
        num_films,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.films_bias = layers.Embedding(num_films, 1) # layer embedding films bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # layer embedding 2
    films_vector = self.films_embedding(inputs[:, 1]) # layer embedding 3
    films_bias = self.films_bias(inputs[:, 1]) # layer embedding 4

    dot_user_films = tf.tensordot(user_vector, films_vector, 2)

    x = dot_user_films + user_bias + films_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_films, 50) # model initialization

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""kita akan menggunakan fungsi callback untuk menghentikan pelatiihan jika nilai RMSE kita kurang dari 0.1 , mengingat pelatihan yang akan kita lakukan sebanyak seratus dan memakan waktu banyak . oleh karena itu jika sudah tercapai nilai RMSE kurang dari 0.1 maka pelatihan akan berhenti dan melanjutkan ke tahap selanjutnya."""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('root_mean_squared_error')<0.1):
      print("mse kurang dari 0.1")
      self.model.stop_training = True
callbacks = myCallback()

# training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    callbacks=[callbacks],
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik"""

# menampilkan nilai RMSE dalam bentuk graph atau visualisai

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""dari hasil diatas terlihat nilai yang sedikit kurang stabil namun tetap menunjuka trend turun dari RMSE baik dari train dan test. oleh karena itu di dapatai hasil untuk nilai RMSE test sebesar 0.27 dan nilai RMSE train adalah 0.17

## Mendapatkan Rekomendasi Resto
"""

# pada bagian ini di tujukan untuk melihat dan meastikan data yang akan di gunakan
preparation

saringan

"""disini kit akan mengambil  data user secara acak dan mendefisnisikan dalam *variable* movie_not_visited"""

movie_df = saringan
df = preparation

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
resto_visited_by_user = df[df.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movie_not_visited = movie_df[~movie_df['id'].isin(resto_visited_by_user.movieId.values)]['id']
movie_not_visited = list(
    set(movie_not_visited)
    .intersection(set(films_to_films_encoded.keys()))
)

movie_not_visited = [[films_to_films_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_resto_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)

# mendapatkan hasil rekomendasi dari  model ini

ratings = model.predict(user_resto_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_resto_ids = [
   films_encoded_to_films.get(movie_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('film with high ratings from user')
print('----' * 8)

top_resto_user = (
    resto_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['id'].isin(top_resto_user)]
for row in movie_df_rows.itertuples():
    print(row.judul, ':', row.genre)

print('----' * 8)
print('Top 10 film recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['id'].isin(recommended_resto_ids)]
for row in recommended_movie.itertuples():
    print(row.judul, ':', row.genre)

